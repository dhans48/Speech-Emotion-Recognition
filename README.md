# Speech-Emotion-Recognition
Environment:

- Python 3.9
- NVIDIA Geforce GTX 1060 6GB
- Conda version 23.1.0

# Dependencies

- [Tensorflow(2.6.0)](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0) for the backend of keras
- [Tensorflow-gpu(2.6.0)](https://pypi.org/project/tensorflow-gpu/2.6.0/) for the backend of keras
- [keras(2.1.3)](https://pypi.org/project/keras/2.13.1/) for building/training the Deep Learning model
- [librosa (0.9.2)](https://github.com/librosa/librosa) for audio resampling
- [scikit learn](https://github.com/scikit-learn/scikit-learn) general purpose machine learning library

# Datasets

- [CREMA-D](https://www.kaggle.com/datasets/ejlok1/cremad) - Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)
- [RAVDESS](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio) - Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)
- [SAVEE](https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee) - Surrey Audio-Visual Expressed Emotion (SAVEE)
- [TESS](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess) - Toronto emotional speech set (TESS)

